{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fb0d82-f5f8-4cf8-b94f-e5c12a5944a9",
   "metadata": {},
   "source": [
    "# Initial Steps\n",
    "- Install Dependencies: \n",
    "- Make sure to install the necessary Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffd0db1-11a9-47be-aeb6-c0180e1501b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: tensorflow in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (70.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorflow) (3.5.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
      "Requirement already satisfied: namex in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn tensorflow nltk joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc491a-e51a-4f58-9cca-49d9c0ce446b",
   "metadata": {},
   "source": [
    "# Read CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ea4505-77dc-47ed-860d-f2fc8718b317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n",
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Jigsaw Toxic Comment Classification Dataset (Kaggle)\n",
    "jigsaw_df = pd.read_csv('../jigsaw_toxic_comment_dataset_train.csv')\n",
    "\n",
    "# Load the Hate Speech and Offensive Language Dataset\n",
    "hate_speech_df = pd.read_csv('../hate_speech_and_offensive_language_labeled_data.csv')\n",
    "\n",
    "print(jigsaw_df.head())\n",
    "print(hate_speech_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c451a1-829f-4cf5-a2ca-979bba22e035",
   "metadata": {},
   "source": [
    "# Combine the datasets\n",
    "- Hate Speech Dataset: Label \"hate speech\" and \"offensive language\" as inappropriate\n",
    "- Jigsaw Dataset: Create \"Inappropriate\" label based on toxic, severe_toxic, obscene, threat, insult, identity_hate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed3f925-406a-4a7d-8c30-656dbb815955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the datasets\n",
    "# Jigsaw Dataset: Create \"Inappropriate\" label based on toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "jigsaw_df['inappropriate'] = jigsaw_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].max(axis=1)\n",
    "jigsaw_df['inappropriate'] = jigsaw_df['inappropriate'].apply(lambda x: 1 if x > 0 else 0)\n",
    "jigsaw_df = jigsaw_df[['comment_text', 'inappropriate']]\n",
    "\n",
    "# Hate Speech Dataset: Label \"hate speech\" and \"offensive language\" as inappropriate\n",
    "hate_speech_df['inappropriate'] = hate_speech_df['class'].apply(lambda x: 1 if x == 0 or x == 1 else 0)\n",
    "hate_speech_df = hate_speech_df[['tweet', 'inappropriate']].rename(columns={'tweet': 'comment_text'})\n",
    "\n",
    "# Combine both datasets\n",
    "combined_df = pd.concat([jigsaw_df, hate_speech_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99665671-e46f-42cb-b54e-2ed3c4725307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text  inappropriate\n",
      "0  Explanation\\nWhy the edits made under my usern...              0\n",
      "1  D'aww! He matches this background colour I'm s...              0\n",
      "2  Hey man, I'm really not trying to edit war. It...              0\n",
      "3  \"\\nMore\\nI can't make any real suggestions on ...              0\n",
      "4  You, sir, are my hero. Any chance you remember...              0\n",
      "                                             comment_text  inappropriate\n",
      "6            COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK              1\n",
      "12      Hey... what is it..\\n@ | talk .\\nWhat is it......              1\n",
      "16      Bye! \\n\\nDon't look, come or think of comming ...              1\n",
      "42      You are gay or antisemmitian? \\n\\nArchangel WH...              1\n",
      "43               FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!              1\n",
      "...                                                   ...            ...\n",
      "184347                                 you're all niggers              1\n",
      "184348  you're such a retard i hope you get type 2 dia...              1\n",
      "184349  you's a muthaf***in lie &#8220;@LifeAsKing: @2...              1\n",
      "184351  young buck wanna eat!!.. dat nigguh like I ain...              1\n",
      "184352              youu got wild bitches tellin you lies              1\n",
      "\n",
      "[36845 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.head())\n",
    "# Assuming your DataFrame is called 'df' and the class label column is 'label'\n",
    "inappropriate_rows = combined_df[combined_df['inappropriate'] == 1]\n",
    "\n",
    "# Print the filtered rows\n",
    "print(inappropriate_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cba86b-500b-417a-8417-1fbead2388ec",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "Text cleaning is crucial to remove unwanted characters, symbols, and patterns that may not contribute to the model's performance. This step also reduces noise in the data.\n",
    "\n",
    "1. Lowercasing: Convert all text to lowercase to avoid treating the same words differently based on capitalization.\n",
    "2. Remove Special Characters: Remove punctuation, special symbols, and numbers as they may not add value for the model.\n",
    "3. Remove URLs and Emails: Links and emails are often not useful and should be removed.\n",
    "4. Remove Stopwords: Stopwords (e.g., \"the\", \"is\", \"in\") are common words that can be removed as they don't carry significant meaning.\n",
    "5. Expand Contractions: Convert contractions like \"don't\" to \"do not\" to standardize the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b168d7a-604d-4560-8cb0-69b5b30440d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saiaravindpunnam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text  \\\n",
      "0  Explanation\\nWhy the edits made under my usern...   \n",
      "1  D'aww! He matches this background colour I'm s...   \n",
      "2  Hey man, I'm really not trying to edit war. It...   \n",
      "3  \"\\nMore\\nI can't make any real suggestions on ...   \n",
      "4  You, sir, are my hero. Any chance you remember...   \n",
      "\n",
      "                                     cleaned_comment  \n",
      "0  explanation edits made username hardcore metal...  \n",
      "1  daww matches background colour I seemingly stu...  \n",
      "2  hey man I really trying edit war guy constantl...  \n",
      "3  cannot make real suggestions improvement wonde...  \n",
      "4                      sir hero chance remember page  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Dictionary of common contractions\n",
    "contractions = {\n",
    "    \"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "    \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "    \"i'd\": \"I would\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"i've\": \"I have\", \"isn't\": \"is not\", \n",
    "    \"it's\": \"it is\", \"let's\": \"let us\", \"mightn't\": \"might not\", \"mustn't\": \"must not\", \n",
    "    \"shan't\": \"shall not\", \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \n",
    "    \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\", \"they'd\": \"they would\", \n",
    "    \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\", \"we'd\": \"we would\", \n",
    "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n",
    "    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"where's\": \"where is\", \n",
    "    \"who'd\": \"who would\", \"who'll\": \"who will\", \"who're\": \"who are\", \"who's\": \"who is\", \n",
    "    \"who've\": \"who have\", \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\", \n",
    "    \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text, contractions)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '', text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"  # other symbols\n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'comment_text' column\n",
    "combined_df['cleaned_comment'] = combined_df['comment_text'].apply(clean_text)\n",
    "\n",
    "# View the cleaned data\n",
    "print(combined_df[['comment_text', 'cleaned_comment']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f362983-9794-4fa2-8875-916503f3ed79",
   "metadata": {},
   "source": [
    "# Text Normalization and Lemmatization\n",
    "- Normalization standardizes the text data by reducing words to their base forms. \n",
    "- Lemmatization ensures that words are reduced to their dictionary form (e.g., \"running\" becomes \"run\").\n",
    "- This helps improve the model’s generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea6e143-6eae-496d-92e6-9f349671bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saiaravindpunnam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     cleaned_comment  \\\n",
      "0  explanation edits made username hardcore metal...   \n",
      "1  daww matches background colour I seemingly stu...   \n",
      "2  hey man I really trying edit war guy constantl...   \n",
      "3  cannot make real suggestions improvement wonde...   \n",
      "4                      sir hero chance remember page   \n",
      "\n",
      "                                  lemmatized_comment  inappropriate  \n",
      "0  explanation edits made username hardcore metal...              0  \n",
      "1  daww match background colour I seemingly stuck...              0  \n",
      "2  hey man I really trying edit war guy constantl...              0  \n",
      "3  cannot make real suggestion improvement wonder...              0  \n",
      "4                      sir hero chance remember page              0  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# Apply lemmatization\n",
    "combined_df['lemmatized_comment'] = combined_df['cleaned_comment'].apply(lemmatize_text)\n",
    "\n",
    "# View the lemmatized data\n",
    "print(combined_df[['cleaned_comment', 'lemmatized_comment', 'inappropriate']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2184943-ed63-47d9-8a9e-9ff13b644f94",
   "metadata": {},
   "source": [
    "# Text Vectorization (TF-IDF (basic) or Word Embeddings (More contextual :TODO ))\n",
    "TF-IDF: Convert the text data into numerical form using TF-IDF (Term Frequency-Inverse Document Frequency), which gives more importance to rare words.\n",
    "## Word Embeddings (Advanced Option): \n",
    "- Instead of TF-IDF, We can use pre-trained word embeddings like Word2Vec, GloVe, or BERT. For example, using pre-trained BERT embeddings will allow the model to capture deeper semantic relationships.\n",
    "- For this, We can utilize Hugging Face's Transformers library to get the BERT embeddings for each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b00834a-db7f-44d8-8390-4023bcddcdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184354, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "\n",
    "# Fit and transform the lemmatized text\n",
    "X_tfidf = tfidf.fit_transform(combined_df['lemmatized_comment'])\n",
    "\n",
    "# Check the shape of the TF-IDF matrix\n",
    "print(X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd50a0-872f-4b06-8ee0-f00ff5a1a3b2",
   "metadata": {},
   "source": [
    "# Splitting the Dataset\n",
    "- Split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aedb0e8-6f5b-4411-b01f-ac38f03c4078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (147483, 10000), Validation set shape: (36871, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tfidf, combined_df['inappropriate'], test_size=0.2, random_state=2)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, Validation set shape: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd505a-c459-4c2f-abc7-a1d39f50afb9",
   "metadata": {},
   "source": [
    "# Building a Simple Logistic Regression Model (Binary Classification Model)\n",
    "- Here, we’ll build a simple logistic regression model to classify comments as toxic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a1b581-5e1e-4ab2-a5db-637c28201377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "  appropriate       0.97      0.96      0.97     29495\n",
      "inappropriate       0.85      0.89      0.87      7376\n",
      "\n",
      "     accuracy                           0.95     36871\n",
      "    macro avg       0.91      0.93      0.92     36871\n",
      " weighted avg       0.95      0.95      0.95     36871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_val, y_pred, target_names=['appropriate', 'inappropriate']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572399b-9d85-4e25-922b-e783e66df9c1",
   "metadata": {},
   "source": [
    "## Metrics Breakdown:\n",
    "\n",
    "1. **Precision**:\n",
    "   - **Appropriate**: 0.97\n",
    "     - This means that out of all the content classified as \"appropriate\", 97% was correctly classified.\n",
    "   - **Inappropriate**: 0.85\n",
    "     - This means that out of all the content classified as \"inappropriate\", 85% was correctly classified.\n",
    "\n",
    "2. **Recall**:\n",
    "   - **Appropriate**: 0.96\n",
    "     - This means that out of all the actual \"appropriate\" content, 96% was correctly identified by the model.\n",
    "   - **Inappropriate**: 0.89\n",
    "     - This means that out of all the actual \"inappropriate\" content, 89% was correctly identified by the model.\n",
    "\n",
    "3. **F1-Score**:\n",
    "   - **Appropriate**: 0.97\n",
    "     - The harmonic mean of precision and recall for the \"appropriate\" class, indicating a balanced performance.\n",
    "   - **Inappropriate**: 0.87\n",
    "     - The harmonic mean of precision and recall for the \"inappropriate\" class, indicating a good balance between precision and recall.\n",
    "\n",
    "4. **Support**:\n",
    "   - **Appropriate**: 29,495\n",
    "     - The number of actual \"appropriate\" samples in the dataset.\n",
    "   - **Inappropriate**: 7,376\n",
    "     - The number of actual \"inappropriate\" samples in the dataset.\n",
    "\n",
    "5. **Accuracy**:\n",
    "   - Overall accuracy: 0.95\n",
    "     - The proportion of correctly classified samples (both \"appropriate\" and \"inappropriate\") out of the total samples.\n",
    "\n",
    "6. **Macro Average**:\n",
    "   - Average performance across all classes, treating each class equally:\n",
    "     - Precision: 0.91\n",
    "     - Recall: 0.93\n",
    "     - F1-Score: 0.92\n",
    "\n",
    "7. **Weighted Average**:\n",
    "   - Average performance across all classes, weighted by the number of instances in each class:\n",
    "     - Precision: 0.95\n",
    "     - Recall: 0.95\n",
    "     - F1-Score: 0.95\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **High Precision for \"Appropriate\"**: Indicates that the model is very good at identifying content as \"appropriate\" when it actually is.\n",
    "- **Good Recall for \"Inappropriate\"**: Shows that the model is effective at identifying a large portion of the actual \"inappropriate\" content.\n",
    "- **Balanced Performance**: The F1-scores suggest a good balance between precision and recall for both classes.\n",
    "\n",
    "Overall, the model performs well in distinguishing between \"appropriate\" and \"inappropriate\" content, with particularly strong performance in identifying \"appropriate\" content and good performance in identifying \"inappropriate\" content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43838a9-003d-43e7-822c-8bf9a1df86a6",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "- **Grid Search** is a technique used for hyperparameter tuning in machine learning. \n",
    "- The goal is to find the best combination of hyperparameters for a given model to optimize its performance.\n",
    "- Grid Search is a powerful tool for optimizing machine learning models by systematically evaluating hyperparameter combinations to find the best settings for your model.\n",
    "- It enhances model performance and ensures more reliable results by using cross-validation to assess each set of hyperparameters.\n",
    "\n",
    "### How Grid Search Works:\n",
    "\n",
    "1. **Define Hyperparameter Space:**\n",
    "   - Specify a set of hyperparameters we want to tune and the range of values for each hyperparameter. For example, We want to tune the regularization strength (`C`) and the `max_df` parameter in a TF-IDF vectorizer.\n",
    "\n",
    "2. **Create a Grid:**\n",
    "   - Grid Search creates a Cartesian product of the hyperparameter values specified. This means it evaluates all possible combinations of the given hyperparameter values.\n",
    "\n",
    "3. **Cross-Validation:**\n",
    "   - For each combination of hyperparameters, Grid Search performs cross-validation. This involves splitting the training data into multiple folds and training the model on different subsets while validating it on the remaining fold. This helps in assessing the performance of each hyperparameter combination more reliably.\n",
    "\n",
    "4. **Evaluate Performance:**\n",
    "   - The model is evaluated based on a specified scoring metric (e.g., accuracy, F1-score, etc.). Grid Search computes the average performance metric across all cross-validation folds for each hyperparameter combination.\n",
    "\n",
    "5. **Select the Best Hyperparameters:**\n",
    "   - The combination of hyperparameters that achieves the best performance (according to the chosen metric) is selected as the optimal set of hyperparameters.\n",
    "\n",
    "6. **Train the Final Model:**\n",
    "   - The model is retrained on the full training dataset using the best hyperparameters found through Grid Search.\n",
    "\n",
    "### Benefits of Grid Search:\n",
    "\n",
    "- **Systematic Search:**\n",
    "  - It exhaustively searches through all possible hyperparameter combinations, ensuring that the best possible set of hyperparameters is found within the specified grid.\n",
    "\n",
    "- **Cross-Validation:**\n",
    "  - By using cross-validation, Grid Search provides a more reliable estimate of model performance and reduces the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab564976-a55c-49b2-b1a1-202f0f60d297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.3/libexec/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__C': 10, 'tfidf__max_df': 0.75, 'tfidf__ngram_range': (1, 2)}\n",
      "Best Model Performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  appropriate       0.97      0.97      0.97     29495\n",
      "inappropriate       0.88      0.89      0.89      7376\n",
      "\n",
      "     accuracy                           0.95     36871\n",
      "    macro avg       0.93      0.93      0.93     36871\n",
      " weighted avg       0.95      0.95      0.95     36871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(combined_df['lemmatized_comment'], combined_df['inappropriate'], test_size=0.2, random_state=2)\n",
    "\n",
    "# Define the pipeline with TF-IDF vectorizer and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # TF-IDF vectorizer\n",
    "    ('clf', LogisticRegression(class_weight='balanced'))  # Classifier\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "parameters = {\n",
    "    'tfidf__max_df': [0.75, 0.85],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "# Initialize Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "# Fit Grid Search to the data\n",
    "grid_search.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "# Retrieve the best model and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model_grid_search.pkl')\n",
    "\n",
    "# Make predictions and evaluate the best model\n",
    "y_pred_raw = best_model.predict(X_val_raw)\n",
    "print(\"Best Model Performance:\")\n",
    "print(classification_report(y_val_raw, y_pred_raw, target_names=['appropriate', 'inappropriate']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "257a985e-7007-4a1a-a7a0-e7191491a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.957036   12.20832779 12.20832779 ... 12.20832779 12.20832779\n",
      " 12.20832779]\n",
      "Text: \"This is fine\" - Prediction: appropriate\n",
      "Text: \"You are a f*cking idiot\" - Prediction: inappropriate\n",
      "Text: \"I love this!\" - Prediction: appropriate\n",
      "Text: \"You b*tch\" - Prediction: appropriate\n",
      "Text: \"fucking bitch\" - Prediction: inappropriate\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the entire pipeline\n",
    "best_model = joblib.load('best_model_grid_search.pkl')\n",
    "\n",
    "print(best_model.named_steps['tfidf'].idf_)\n",
    "\n",
    "# Assuming best_model is your fitted pipeline from GridSearchCV\n",
    "\n",
    "# Define test texts\n",
    "test_texts = [\"This is fine\", \"You are a f*cking idiot\", \"I love this!\", \"You b*tch\", \"fucking bitch\"]\n",
    "\n",
    "# Make predictions using the best pipeline model\n",
    "y_pred_test = best_model.predict(test_texts)\n",
    "\n",
    "# Print predictions\n",
    "for text, prediction in zip(test_texts, y_pred_test):\n",
    "    print(f'Text: \"{text}\" - Prediction: {\"inappropriate\" if prediction == 1 else \"appropriate\"}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c993ad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.5' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Assuming you have your classifier and test data ready\n",
    "# Let's assume 'y_val' are the true labels and 'y_pred_proba' are predicted probabilities for class 1 (inappropriate class)\n",
    "\n",
    "# Get predicted probabilities for the positive class (inappropriate content)\n",
    "y_pred_proba = best_model.predict_proba(X_val_raw)[:, 1]\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, thresholds = roc_curve(y_val_raw, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
